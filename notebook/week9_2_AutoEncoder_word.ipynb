{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A->a B->?, Let's guess '?'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n"
     ]
    }
   ],
   "source": [
    "with open('../../dataset/NLP/tokens_wiki.txt', encoding='UTF-8') as f:\n",
    "    token = f.read()\n",
    "    \n",
    "token = token.split()\n",
    "print(token[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 시퀀스 만들기\n",
    "def generate_sorted_words(tokens):\n",
    "    counter = Counter(tokens)\n",
    "    l = []\n",
    "    for (word, cnt) in counter.most_common():\n",
    "        l.append(word)\n",
    "    return l\n",
    "\n",
    "def generate_word2code(sorted_words): \n",
    "    d = {}\n",
    "    for idx, word in enumerate(sorted_words):\n",
    "        d[word] = idx\n",
    "    return d\n",
    "\n",
    "def convert_tokens_to_codes(tokens, word2code):\n",
    "    return [word2code[word] for word in tokens]\n",
    "    \n",
    "\n",
    "# 시퀀스 기반으로 word-context 만들기\n",
    "def generate_word_by_context(\n",
    "    codes,                    # 시퀀스 (정수)\n",
    "    max_vocab_words = 1000,   # 중심어 가능 코드 (정수코드 최댓값, 행)\n",
    "    max_context_words = 1000, # 스캔 가능 코드 (정수코드 최댓값, 열)\n",
    "    context_size = 2,         # 좌우 단어 개수\n",
    "    weight_by_distance = True # 거리 고려 유무\n",
    "):\n",
    "    context = [[0 for _ in range(max_context_words)] for _ in range(max_vocab_words)]\n",
    "    for idx, number in enumerate(codes):\n",
    "        if number >= max_vocab_words:\n",
    "            continue\n",
    "        left = max(0, idx-context_size)\n",
    "        right = min(len(codes)-1, idx+context_size)\n",
    "        for i in range(left, right+1):\n",
    "            if i == idx:\n",
    "                continue\n",
    "            if codes[i] >= max_context_words:\n",
    "                continue\n",
    "            if weight_by_distance:\n",
    "                context[number][codes[i]] += 1 / abs(idx-i)\n",
    "            else:\n",
    "                context[number][codes[i]] += 1\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86321.49999993434 140434.916666573 146811.91666662702 218371.91666685182 73127.83333334778 104576.24999995445 68207.66666668332 17028.58333333861 29950.083333325125 24884.499999997257 \n",
      "140434.916666573 97901.66666659727 34584.74999998814 50216.9166666931 112055.83333327656 51930.3333333527 27496.83333332475 38234.999999998705 18023.25000000103 23744.999999997235 \n",
      "146811.91666662702 34584.74999998814 1513.9999999999886 40901.50000000441 25339.333333326445 51535.66666667933 18721.083333334413 23911.74999999613 16202.08333333642 10781.083333333627 \n",
      "218371.91666685182 50216.9166666931 40901.50000000441 17275.166666670786 28603.916666655427 20769.999999998257 12971.250000002641 35277.083333327886 9844.50000000001 8918.24999999945 \n",
      "73127.83333334778 112055.83333327656 25339.333333326445 28603.916666655427 9421.166666666344 20517.166666665733 15447.750000003129 15304.500000001972 7141.916666666315 6216.41666666631 \n",
      "104576.24999995445 51930.3333333527 51535.66666667933 20769.999999998257 20517.166666665733 8857.666666666159 11249.833333334082 19097.75000000048 6963.999999999538 4804.833333333354 \n",
      "68207.66666668332 27496.83333332475 18721.083333334413 12971.250000002641 15447.750000003129 11249.833333334082 7719.499999999178 16196.333333335038 7464.416666666176 5520.999999999854 \n",
      "17028.58333333861 38234.999999998705 23911.74999999613 35277.083333327886 15304.500000001972 19097.75000000048 16196.333333335038 4850.6666666666915 14256.666666667637 17310.916666667064 \n",
      "29950.083333325125 18023.25000000103 16202.08333333642 9844.50000000001 7141.916666666315 6963.999999999538 7464.416666666176 14256.666666667637 791.666666666672 2980.8333333333912 \n",
      "24884.499999997257 23744.999999997235 10781.083333333627 8918.24999999945 6216.41666666631 4804.833333333354 5520.999999999854 17310.916666667064 2980.8333333333912 8032.833333333188 \n"
     ]
    }
   ],
   "source": [
    "sorted_word = generate_sorted_words(token)\n",
    "word2code = generate_word2code(sorted_word)\n",
    "codes = convert_tokens_to_codes(token, word2code)\n",
    "context = generate_word_by_context(codes, max_vocab_words=10000, max_context_words=1000, context_size=4, weight_by_distance=True)\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        print(context[i][j], end=' ')\n",
    "    print('')\n",
    "    \n",
    "# print(token[:10])\n",
    "# print(codes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "\tdef __init__(self, context_words, d):\n",
    "\t\tsuper(AutoEncoder, self).__init__()\n",
    "\t\tself.encoder = nn.Sequential(\n",
    "\t\t\tnn.Linear(context_words, int((context_words+d)/2), bias=False),\n",
    "\t\t\tnn.Linear(int((context_words+d)/2), d, bias=False)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.decoder = nn.Sequential(\n",
    "\t\t\tnn.Linear(d, int((context_words+d)/2), bias=False),\n",
    "\t\t\tnn.Linear(int((context_words+d)/2), context_words, bias=False)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.encoder(x)\n",
    "\t\tx = self.decoder(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 600]         600,000\n",
      "            Linear-2                  [-1, 200]         120,000\n",
      "            Linear-3                  [-1, 600]         120,000\n",
      "            Linear-4                 [-1, 1000]         600,000\n",
      "================================================================\n",
      "Total params: 1,440,000\n",
      "Trainable params: 1,440,000\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 5.49\n",
      "Estimated Total Size (MB): 5.52\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "d = 200\n",
    "model = AutoEncoder(context_words=1000, d=d)\n",
    "model = model.to('cuda')\n",
    "summary(model, (1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "batch = 250\n",
    "optim = torch.optim.Adam(params=model.parameters(), lr=10**-6)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.tensor(context)\n",
    "context = nn.functional.normalize(context, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.040363\n",
      "0.040064\n",
      "0.039733\n",
      "0.039394\n",
      "0.039055\n",
      "0.038712\n",
      "0.038365\n",
      "0.038009\n",
      "0.037643\n",
      "0.037262\n",
      "0.036865\n",
      "0.036449\n",
      "0.036013\n",
      "0.035554\n",
      "0.03507\n",
      "0.034563\n",
      "0.03403\n",
      "0.033472\n",
      "0.032888\n",
      "0.03228\n",
      "0.031647\n",
      "0.030992\n",
      "0.030314\n",
      "0.029615\n",
      "0.028898\n",
      "0.028163\n",
      "0.027414\n",
      "0.026651\n",
      "0.025878\n",
      "0.025097\n",
      "0.02431\n",
      "0.02352\n",
      "0.022729\n",
      "0.021941\n",
      "0.021157\n",
      "0.020381\n",
      "0.019617\n",
      "0.018872\n",
      "0.01815\n",
      "0.017458\n",
      "0.0168\n",
      "0.016183\n",
      "0.015614\n",
      "0.015098\n",
      "0.014638\n",
      "0.014235\n",
      "0.013891\n",
      "0.013602\n",
      "0.013371\n",
      "0.013203\n",
      "0.013102\n",
      "0.013068\n",
      "0.013102\n",
      "0.01319\n",
      "0.013324\n",
      "0.013513\n",
      "0.013757\n",
      "0.014043\n",
      "0.014361\n",
      "0.014698\n",
      "0.015049\n",
      "0.015402\n",
      "0.01574\n",
      "0.016066\n",
      "0.016383\n",
      "0.016678\n",
      "0.016934\n",
      "0.017149\n",
      "0.01731\n",
      "0.017399\n",
      "0.017415\n",
      "0.017367\n",
      "0.017269\n",
      "0.017122\n",
      "0.016924\n",
      "0.016677\n",
      "0.016386\n",
      "0.016059\n",
      "0.015715\n",
      "0.015367\n",
      "0.015026\n",
      "0.014704\n",
      "0.014402\n",
      "0.01412\n",
      "0.013858\n",
      "0.013618\n",
      "0.013401\n",
      "0.013215\n",
      "0.013067\n",
      "0.012963\n",
      "0.012899\n",
      "0.012871\n",
      "0.012872\n",
      "0.012898\n",
      "0.012945\n",
      "0.01301\n",
      "0.013092\n",
      "0.01319\n",
      "0.013304\n",
      "0.013433\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for epoch_cnt in range(epoch):\n",
    "\tprint_loss = 0\n",
    "\n",
    "\tfor batch_cnt in range(int(10000/batch)):\n",
    "\t\tinput = context[batch_cnt * batch : (batch_cnt + 1) * batch]\n",
    "\t\tinput = input.to('cuda')\n",
    "\t\toutput = model(input)\n",
    "\n",
    "\t\tloss = loss_fn(output, input)\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\toptim.step()\n",
    "\t\tprint_loss += loss.item()\n",
    "\t\t\n",
    "\tprint(round(print_loss, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9978], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "beijing -> china as ottawa -> minnesota\n"
     ]
    }
   ],
   "source": [
    "# france : paris = germanry : ?\n",
    "\n",
    "word1 = \"beijing\"\n",
    "word2 = \"china\"\n",
    "word3 = \"ottawa\"\n",
    "\n",
    "code_word1 = word2code[word1]\n",
    "code_word2 = word2code[word2]\n",
    "code_word3 = word2code[word3]\n",
    "\n",
    "context_word1 = nn.functional.normalize(context[code_word1].to('cuda').unsqueeze(0))\n",
    "context_word2 = nn.functional.normalize(context[code_word2].to('cuda').unsqueeze(0))\n",
    "context_word3 = nn.functional.normalize(context[code_word3].to('cuda').unsqueeze(0))\n",
    "\n",
    "word4_embedded = model.encoder(context_word2) - model.encoder(context_word1) + model.encoder(context_word3)\n",
    "\n",
    "max_cos_sim = -1\n",
    "idx = 0\n",
    "for i in range(10000):\n",
    "\tdata = context[i]\n",
    "\tcos_sim = nn.functional.cosine_similarity(word4_embedded, model.encoder(data.to('cuda')).unsqueeze(0))\n",
    "\tif cos_sim>max_cos_sim:\n",
    "\t\tmax_cos_sim=cos_sim\n",
    "\t\tidx = i\n",
    "print(max_cos_sim)\n",
    "for item in word2code:\n",
    "\tif word2code[item] == idx:\n",
    "\t\tprint(f\"{word1} -> {word2} as {word3} -> {item}\")\n",
    "\t\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8172621334357792\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 입력 데이터 전처리\n",
    "with open('../../dataset/NLP/questions-words.txt') as f:\n",
    "\ttxt = f.read()\n",
    "txt = txt.split('\\n')\n",
    "txt = [s.lower().split() for s in txt if not s.startswith(':')]\n",
    "\n",
    "input_words = []\n",
    "for n in range(len(txt)):\n",
    "    word1, word2, word3, word4 = txt[n]\n",
    "    if word1 not in word2code or word2 not in word2code or word3 not in word2code or word4 not in word2code:\n",
    "        continue\n",
    "    code_word1 = word2code[word1]\n",
    "    code_word2 = word2code[word2]\n",
    "    code_word3 = word2code[word3]\n",
    "    if code_word1 >= 10000 or code_word2 >= 10000 or code_word3 >= 10000:\n",
    "        continue\n",
    "    input_words.append((code_word1, code_word2, code_word3, word4))\n",
    "\n",
    "# GPU로 모델 이동\n",
    "model.to('cuda')\n",
    "\n",
    "# 입력 데이터를 GPU로 이동\n",
    "input_data = [(context[code_word1].to('cuda'), context[code_word2].to('cuda'), context[code_word3].to('cuda'), word4) for code_word1, code_word2, code_word3, word4 in input_words]\n",
    "\n",
    "# 모든 데이터를 한 번에 모아서 모델에 입력\n",
    "input_word2_embedded = torch.stack([model.encoder(context_word2) for context_word1, context_word2, context_word3, word4 in input_data])\n",
    "input_word1_embedded = torch.stack([model.encoder(context_word1) for context_word1, context_word2, context_word3, word4 in input_data])\n",
    "input_word3_embedded = torch.stack([model.encoder(context_word3) for context_word1, context_word2, context_word3, word4 in input_data])\n",
    "\n",
    "# 유사도 계산을 위한 데이터를 GPU로 이동\n",
    "context_embeddings = context.to('cuda')\n",
    "\n",
    "# 모델 평가\n",
    "correct = 0\n",
    "for idx, (input_word1, input_word2, input_word3, word4) in enumerate(input_data):\n",
    "    word4_embedded = input_word2_embedded[idx] - input_word1_embedded[idx] + input_word3_embedded[idx]\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(word4_embedded.unsqueeze(0), model.encoder(context_embeddings))\n",
    "    max_cos_sim, max_idx = torch.max(cos_sim, dim=0)\n",
    "\n",
    "    word4_idx = max_idx.item()\n",
    "    ans = ''\n",
    "    for item, code in word2code.items():\n",
    "        if code == word4_idx:\n",
    "            ans = item\n",
    "            break\n",
    "    #print(word4.lower(), ans)\n",
    "    if word4.lower() == ans:\n",
    "        correct += 1\n",
    "\n",
    "print(correct / len(input_data) * 100)\n",
    "print(correct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
