{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neo64\\anaconda3\\envs\\ml\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\neo64\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "vgg16 = torchvision.models.vgg16(weights=True)\n",
    "# vgg16.features\n",
    "# vgg16.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "\tdef __init__(self, n_class=21):\n",
    "\t\tsuper(FCN, self).__init__()\n",
    "\t\tself.conv1_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "\t\tself.relu1_1 = nn.ReLU(inplace=True)\n",
    "\t\tself.conv1_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "\t\tself.relu1_2 = nn.ReLU(inplace=True)\n",
    "\t\tself.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "\t\t\n",
    "\t\tself.conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "\t\tself.relu2_1 = nn.ReLU(inplace=True)\n",
    "\t\tself.conv2_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "\t\tself.relu2_2 = nn.ReLU(inplace=True)\n",
    "\t\tself.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "\n",
    "\t\tself.conv3_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "\t\tself.relu3_1 = nn.ReLU(inplace=True)\n",
    "\t\tself.conv3_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "\t\tself.relu3_2 = nn.ReLU(inplace=True)\n",
    "\t\tself.conv3_3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "\t\tself.relu3_3 = nn.ReLU(inplace=True)\n",
    "\t\tself.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "\n",
    "\t\tself.conv4_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "\t\tself.relu4_1 = nn.ReLU(inplace=True)\n",
    "\t\tself.conv4_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\t\tself.relu4_2 = nn.ReLU(inplace=True)\n",
    "\t\tself.conv4_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\t\tself.relu4_3 = nn.ReLU(inplace=True)\n",
    "\t\tself.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "\n",
    "\t\tself.conv5_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\t\tself.relu5_1 = nn.ReLU(inplace=True)\n",
    "\t\tself.conv5_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\t\tself.relu5_2 = nn.ReLU(inplace=True)\n",
    "\t\tself.conv5_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "\t\tself.relu5_3 = nn.ReLU(inplace=True)\n",
    "\t\tself.maxpool5 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "\n",
    "\t\tself.conv6 = nn.Conv2d(in_channels=512, out_channels=4096, kernel_size=1, padding=0)\n",
    "\t\tself.relu6 = nn.ReLU(inplace=True)\n",
    "\t\tself.drop6 = nn.Dropout2d()\n",
    "\n",
    "\t\tself.conv7 = nn.Conv2d(in_channels=4096, out_channels=4096, kernel_size=1, padding=0)\n",
    "\t\tself.relu7 = nn.ReLU(inplace=True)\n",
    "\t\tself.drop7 = nn.Dropout2d()\n",
    "\n",
    "\t\tself.conv8 = nn.Conv2d(in_channels=4096, out_channels=n_class, kernel_size=1, padding=0)\n",
    "\n",
    "\n",
    "\t\tself.x2_upsample = nn.ConvTranspose2d(in_channels=n_class, out_channels=n_class, kernel_size=4, stride=2)\n",
    "\t\tself.x8_upsample = nn.ConvTranspose2d(in_channels=n_class, out_channels=n_class, kernel_size=16, stride=8)\n",
    "\t\t#self.x16_upsample = nn.ConvTranspose2d(in_channels=n_class, out_channels=n_class, kernel_size=17, stride=16, padding=1, output_padding=1)\n",
    "\t\t#self.x32_upsample = nn.ConvTranspose2d(in_channels=n_class, out_channels=n_class, kernel_size=33, stride=32, padding=1, output_padding=1)\n",
    "\n",
    "\n",
    "\t\tself.pool4_pred = nn.Conv2d(in_channels=512, out_channels=n_class, kernel_size=1, padding=0)\n",
    "\t\tself.pool3_pred = nn.Conv2d(in_channels=256, out_channels=n_class, kernel_size=1, padding=0)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.relu1_1(self.conv1_1(x))\n",
    "\t\tx = self.relu1_2(self.conv1_2(x))\n",
    "\t\tx = self.maxpool1(x)\n",
    "\n",
    "\t\tx = self.relu2_1(self.conv2_1(x))\n",
    "\t\tx = self.relu2_2(self.conv2_2(x))\n",
    "\t\tx = self.maxpool2(x)\n",
    "\n",
    "\t\tx = self.relu3_1(self.conv3_1(x))\n",
    "\t\tx = self.relu3_2(self.conv3_2(x))\n",
    "\t\tx = self.relu3_3(self.conv3_3(x))\n",
    "\t\tx = self.maxpool3(x)\n",
    "\t\tpool3_pred = self.pool3_pred(x)\n",
    "\n",
    "\t\tx = self.relu4_1(self.conv4_1(x))\n",
    "\t\tx = self.relu4_2(self.conv4_2(x))\n",
    "\t\tx = self.relu4_3(self.conv4_3(x))\n",
    "\t\tx = self.maxpool4(x)\n",
    "\t\tpool4_pred = self.pool4_pred(x)\n",
    "\t\t\n",
    "\t\tx = self.relu5_1(self.conv5_1(x))\n",
    "\t\tx = self.relu5_2(self.conv5_2(x))\n",
    "\t\tx = self.relu5_3(self.conv5_3(x))\n",
    "\t\tx = self.maxpool5(x)\n",
    "\n",
    "\n",
    "\t\tx = self.drop6(self.relu6(self.conv6(x)))\n",
    "\t\tx = self.drop7(self.relu7(self.conv7(x)))\n",
    "\t\tx = self.conv8(x)\n",
    "\n",
    "\t\tus = self.x2_upsample(x)\n",
    "\t\tus = us[ :, : , 1:-1, 1:-1]\n",
    "\t\t#FCN_32s = self.x32_upsample(x)\n",
    "\t\t\n",
    "\t\tus_with_pool4_prediction = us + pool4_pred\n",
    "\t\t#FCN_16s = self.x16_upsample(us_with_pool4_prediction)\n",
    "\t\t\n",
    "\t\tus2 = self.x2_upsample(us_with_pool4_prediction)\n",
    "\t\tus2 = us2[ :, : , 1:-1, 1:-1]\n",
    "\t\tus2_with_pool3_prediction = us2 + pool3_pred\n",
    "\t\tFCN_8s = self.x8_upsample(us2_with_pool3_prediction)\n",
    "\t\tresult = FCN_8s[ :, :, 4:-4, 4:-4]\n",
    "\n",
    "\t\treturn result\n",
    "\t\n",
    "\tdef copy_params_from_vgg16(self, vgg16):\n",
    "\t\tfeatures = [\n",
    "\t\t\tself.conv1_1, self.relu1_1,\n",
    "\t\t\tself.conv1_2, self.relu1_2,\n",
    "\t\t\tself.maxpool1,\n",
    "\t\t\tself.conv2_1, self.relu2_1,\n",
    "\t\t\tself.conv2_2, self.relu2_2,\n",
    "\t\t\tself.maxpool2,\n",
    "\t\t\tself.conv3_1, self.relu3_1,\n",
    "\t\t\tself.conv3_2, self.relu3_2,\n",
    "\t\t\tself.conv3_3, self.relu3_3,\n",
    "\t\t\tself.maxpool3,\n",
    "\t\t\tself.conv4_1, self.relu4_1,\n",
    "\t\t\tself.conv4_2, self.relu4_2,\n",
    "\t\t\tself.conv4_3, self.relu4_3,\n",
    "\t\t\tself.maxpool4,\n",
    "\t\t\tself.conv5_1, self.relu5_1,\n",
    "\t\t\tself.conv5_2, self.relu5_2,\n",
    "\t\t\tself.conv5_3, self.relu5_3,\n",
    "\t\t\tself.maxpool5,\n",
    "\t\t]\n",
    "\t\tfor l1, l2 in zip(vgg16.features, features):\n",
    "\t\t\tprint(l1)\n",
    "\t\t\tif isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n",
    "\t\t\t\tassert l1.weight.size() == l2.weight.size()\n",
    "\t\t\t\tassert l1.bias.size() == l2.bias.size()\n",
    "\t\t\t\tl2.weight.data.copy_(l1.weight.data)\n",
    "\t\t\t\tl2.bias.data.copy_(l1.bias.data)\n",
    "\t\t\n",
    "\t\tfor layer in features:\n",
    "\t\t\tfor param in layer.parameters():\n",
    "\t\t\t\tparam.requires_grad = False\n",
    "\t\t\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "\tdef __init__(self, transform_data, transform_target):\n",
    "\t\tself.transform_data = transform_data\n",
    "\t\tself.transform_target = transform_target\n",
    "\n",
    "\tdef __call__(self, image, target):\n",
    "\t\tfor t in self.transform_data:\n",
    "\t\t\timage = t(image)\n",
    "\n",
    "\t\tfor t in self.transform_target:\n",
    "\t\t\ttarget = t(target)\n",
    "\t\t\t\n",
    "\t\treturn image, target\n",
    "\t\n",
    "class Remove255(object):\n",
    "\tdef __call__(self, tensor):\n",
    "\t\treturn torch.where(tensor>20, torch.tensor(0), tensor)\n",
    "\n",
    "transform_target = []\n",
    "transform_target.append(transforms.Resize((224,224)))\n",
    "transform_target.append(transforms.PILToTensor())\n",
    "transform_target.append(Remove255())\n",
    "transform_data = []\n",
    "transform_data.append(transforms.Resize((224,224)))\n",
    "transform_data.append(transforms.ToTensor())\n",
    "transform_data.append(transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)))\n",
    "# transform_data.append(transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
    "transform = Compose(transform_data, transform_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC2012_train = torchvision.datasets.VOCSegmentation('VOC2012_seg', image_set=\"train\", transforms=transform)\n",
    "VOC2012_test = torchvision.datasets.VOCSegmentation('VOC2012_seg', image_set=\"val\", transforms=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "trainloader = DataLoader(VOC2012_train, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(VOC2012_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.1254,  0.1597,  0.1597,  ...,  1.0159,  0.9988,  0.9988],\n",
      "         [ 0.1426,  0.1768,  0.1768,  ...,  1.0331,  1.0159,  1.0159],\n",
      "         [ 0.1768,  0.1768,  0.1939,  ...,  1.0502,  1.0331,  1.0159],\n",
      "         ...,\n",
      "         [-1.9809, -1.9467, -1.9124,  ..., -1.9467, -1.9295, -1.9124],\n",
      "         [-1.9980, -1.9467, -1.9295,  ..., -1.8439, -1.8610, -1.8268],\n",
      "         [-2.0152, -1.9638, -1.9295,  ..., -1.8610, -1.8610, -1.8439]],\n",
      "\n",
      "        [[ 1.3431,  1.3782,  1.3957,  ...,  2.0434,  2.0259,  2.0259],\n",
      "         [ 1.3606,  1.3957,  1.4132,  ...,  2.0609,  2.0434,  2.0434],\n",
      "         [ 1.3957,  1.3957,  1.4307,  ...,  2.0784,  2.0609,  2.0434],\n",
      "         ...,\n",
      "         [-1.7731, -1.7731, -1.7556,  ..., -1.5280, -1.5280, -1.5105],\n",
      "         [-1.7556, -1.7731, -1.7556,  ..., -1.4230, -1.4405, -1.4230],\n",
      "         [-1.7556, -1.7731, -1.7556,  ..., -1.4230, -1.4580, -1.4580]],\n",
      "\n",
      "        [[ 1.8208,  1.8557,  1.8731,  ...,  2.3786,  2.3960,  2.3960],\n",
      "         [ 1.8383,  1.8731,  1.8905,  ...,  2.3960,  2.4134,  2.4134],\n",
      "         [ 1.8731,  1.8731,  1.9080,  ...,  2.4134,  2.4308,  2.4134],\n",
      "         ...,\n",
      "         [-1.5953, -1.5256, -1.4559,  ..., -1.2293, -1.2293, -1.2119],\n",
      "         [-1.5430, -1.4907, -1.4559,  ..., -1.1247, -1.1421, -1.1247],\n",
      "         [-1.5256, -1.4907, -1.4384,  ..., -1.1421, -1.1421, -1.1247]]]), tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8))\n"
     ]
    }
   ],
   "source": [
    "print(VOC2012_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
     ]
    }
   ],
   "source": [
    "model = FCN()\n",
    "model.to('cuda')\n",
    "model.copy_params_from_vgg16(vgg16)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18           [-1, 21, 28, 28]           5,397\n",
      "           Conv2d-19          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-20          [-1, 512, 28, 28]               0\n",
      "           Conv2d-21          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-22          [-1, 512, 28, 28]               0\n",
      "           Conv2d-23          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-24          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-25          [-1, 512, 14, 14]               0\n",
      "           Conv2d-26           [-1, 21, 14, 14]          10,773\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "           Conv2d-31          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-32          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-33            [-1, 512, 7, 7]               0\n",
      "           Conv2d-34           [-1, 4096, 7, 7]       2,101,248\n",
      "             ReLU-35           [-1, 4096, 7, 7]               0\n",
      "        Dropout2d-36           [-1, 4096, 7, 7]               0\n",
      "           Conv2d-37           [-1, 4096, 7, 7]      16,781,312\n",
      "             ReLU-38           [-1, 4096, 7, 7]               0\n",
      "        Dropout2d-39           [-1, 4096, 7, 7]               0\n",
      "           Conv2d-40             [-1, 21, 7, 7]          86,037\n",
      "  ConvTranspose2d-41           [-1, 21, 16, 16]           7,077\n",
      "  ConvTranspose2d-42           [-1, 21, 30, 30]           7,077\n",
      "  ConvTranspose2d-43         [-1, 21, 232, 232]         112,917\n",
      "================================================================\n",
      "Total params: 33,826,526\n",
      "Trainable params: 19,111,838\n",
      "Non-trainable params: 14,714,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 236.56\n",
      "Params size (MB): 129.04\n",
      "Estimated Total Size (MB): 366.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(IoULoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        #intersection is equivalent to True Positive count\n",
    "        #union is the mutually inclusive area of all labels & predictions \n",
    "        intersection = (inputs * targets).sum()\n",
    "        total = (inputs + targets).sum()\n",
    "        union = total - intersection \n",
    "        \n",
    "        IoU = (intersection + smooth)/(union + smooth)\n",
    "                \n",
    "        return 1 - IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = (10**-3)*5\n",
    "momentum = 0.9\n",
    "weight_decay = 5**-4\n",
    "optim = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish epoch : 0 (lr = 0.001)\n",
      "loss : 1.0939750671386719\n",
      "finish epoch : 1 (lr = 0.001)\n",
      "loss : 0.9760077595710754\n",
      "finish epoch : 2 (lr = 0.001)\n",
      "loss : 1.0911544561386108\n",
      "finish epoch : 3 (lr = 0.001)\n",
      "loss : 1.1510682106018066\n",
      "finish epoch : 4 (lr = 0.001)\n",
      "loss : 1.046970248222351\n",
      "finish epoch : 5 (lr = 0.001)\n",
      "loss : 1.1190918684005737\n",
      "finish epoch : 6 (lr = 0.001)\n",
      "loss : 0.836222767829895\n",
      "finish epoch : 7 (lr = 0.001)\n",
      "loss : 0.9739610552787781\n",
      "finish epoch : 8 (lr = 0.001)\n",
      "loss : 1.01797616481781\n",
      "finish epoch : 9 (lr = 0.001)\n",
      "loss : 1.1496601104736328\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[594], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(save_path))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_cnt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[1;32m----> 9\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m index, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader):\n\u001b[0;32m     10\u001b[0m \t\toptim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     11\u001b[0m \t\tdata \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\neo64\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\neo64\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\neo64\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\neo64\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\neo64\\anaconda3\\envs\\ml\\lib\\site-packages\\torchvision\\datasets\\voc.py:168\u001b[0m, in \u001b[0;36mVOCSegmentation.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m        index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m        tuple: (image, target) where target is the image segmentation.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     target \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks[index])\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\neo64\\anaconda3\\envs\\ml\\lib\\site-packages\\PIL\\Image.py:922\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    876\u001b[0m ):\n\u001b[0;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\neo64\\anaconda3\\envs\\ml\\lib\\site-packages\\PIL\\ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "save_path = 'fcn/final19.pt'\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "for epoch_cnt in range(epoch):\n",
    "\n",
    "\tfor index, (data, target) in enumerate(trainloader):\n",
    "\t\toptim.zero_grad()\n",
    "\t\tdata = data.to('cuda')\n",
    "\t\ttarget = target.to('cuda')\n",
    "\t\ttarget = torch.squeeze(target)\n",
    "\t\ttarget = target.long()\n",
    "\t\tprediction = model(data)\n",
    "\t\t# print(prediction.shape, target.shape)\n",
    "\t\tloss = loss_fn(prediction, target)\n",
    "\t\tloss.backward()\n",
    "\t\toptim.step()\n",
    "\ttorch.save(model.state_dict(), save_path)\n",
    "\tprint(f\"finish epoch : {epoch_cnt} (lr = {lr})\")\n",
    "\tprint(f\"loss : {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neo64\\AppData\\Local\\Temp\\ipykernel_45692\\2029567585.py:25: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = plt.cm.get_cmap('tab20', 20)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAESCAYAAADXBC7TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuWElEQVR4nO3deVRUd94m8KdUNm0WQaUo2Wqiidg4dqK+KsElLhhOa0TTx43Toz0kRo3MwS2RN5mJvscRO0bNtHbI4MtAFpDknATSdjSKmqC0oY/a5I0LKiaFQEK9tCgUKpt65w9SV4q9oG7dpZ7POXVC1b1V9b0p/PLc3/3dWzpBEAQQERERKcgAuQsgIiIiao8BhYiIiBSHAYWIiIgUhwGFiIiIFIcBhYiIiBSHAYWIiIgUhwGFiIiIFIcBhYiIiBSHAYWIiIgUhwGFiIiIFEfWgPLee+/BaDTC09MTEyZMwJkzZ+Qsh4hUgH2DyDXIFlA++eQTJCUl4Y033kBxcTGmTZuG2NhYlJeXy1USESkc+waR69DJ9WWBkydPxjPPPIPU1FTxsYiICMTFxSElJUWOkohI4dg3iFzHIDnetLm5GRcuXMDWrVttHo+JicHZs2c7rN/U1ISmpibx/qNHj3D79m0EBARAp9NJXi8RdSQIAurr62EwGDBggPSDsfb2DYC9g0hp7OkbsgSUW7du4eHDhwgMDLR5PDAwEGazucP6KSkp2L59u7PKIyI7VFRUIDg4WPL3sbdvAOwdRErVm74hS0Cxar8HIwhCp3s1ycnJ2Lhxo3i/rq4OoaGhGLk2EwM8BkteJxF19KjpPn5KXQVvb2+nvm9v+wbA3kGkNPb0DVkCyrBhwzBw4MAOez3V1dUd9o4AwMPDAx4eHh0eH+AxmE2GSGbOOlRib98A2DuIlKo3fUOWs3jc3d0xYcIE5Ofn2zyen5+PqKgoOUoiIoVj3yByLbId4tm4cSN+//vfY+LEiZg6dSrS0tJQXl6ONWvWyFUSESkc+waR65AtoCxduhQ1NTX4t3/7N1RVVSEyMhJHjhxBWFiYXCURkcKxbxC5Dlknya5btw7r1q2TswQiUhn2DSLXwO/iISIiIsVhQCEiIiLFYUAhIiIixWFAISIiIsVhQCEiIiLFYUAhIiIixWFAISIiIsVhQCEiIiLFYUAhIiIixWFAISIiIsVhQCEiIiLFYUAhIiIixWFAISIiIsVhQCEiIiLFYUAhIiIixWFAISIiIsVhQCEiIiLFYUAhIiIixWFAISIiIsVhQCEiIiLFYUAhIiIixWFAISIiIsVhQCEiIiLFYUAhIiIixRkkdwFERKQOZXMKcXLApwCAZ5vyMKagXOaKSMsYUIiIqEfWcFIecgYAUA7gKsCQQpLhIR4iIurRuJ/+KoYTq0OjAnB1RqhMFZHWMaAQEVG3yuYU4t2Q+50u+5tHXOs6nisYVsiheIjHxZTNKUTRrxYiOK8F0bDIXQ4RKVz7QzvtlYecQU5cOYpwDCUBATzsQw7DgKJx3hFbcfCpg+L9k/c/RXlAIkoSgJya1ibCsEJE7Vl7x8n7XYcTq5KAxyMnh0YxpJBjMKBo3PoZ/w0lLW2GXQMeNxprUylJAArTf2BIISIAreHk3ZD7rT0ioPtw0pnK0flAwVMSVEauhHNQNMw7YitG/fh+r9bNT3gChfCRuCIiUjprOOlp1IRIagwoGmUdnrWnyTCkELku74itWOV5ziHh5IywjL2E+o0BRYOs4aTtceHeYkghcj3WUZOw+NUOGzmZOM/bIa9DrosBRWP6E06sGl8658CKiEipvCO24uqMULtHW3uSXFmN7cZ8h70euSZOktWYT1MeoOgwr0VARN1LXnMF480HcSggAECAQ1+7MOQU5rw/1qGvSa6HIygaYd0TqsvM6PdrnRGW4eqMUFydEcrDPUQa1BpOpvVrpJVIahxB0QDrYR1H7gkdGtX6OsmeD+F5jKcfE2mFlOEkubIaX69djhYAiHvH4a9ProUBRQMuNq5DBveEiKgHUo+cFIacwhwGE3IQhx/i2bZtG3Q6nc1Nr9eLywVBwLZt22AwGODl5YWZM2fi8uXLji7DZXhHbEXR+Kclee3kymqYr3+LQvgga3SZJO9BBLBvOAMP65DaSDIH5de//jWqqqrE28WLF8Vlb7/9Nvbu3YsDBw7g3Llz0Ov1mDt3Lurr66UoRfMuNq6TdG8o3BSKYOMlPPuA05VIWuwb0nFGOEmurEbL/IM9r0jUS5L81Rk0aBD0er14Gz58OIDWvaB3330Xb7zxBhYvXozIyEh88MEHuH//PrKzs6UoRdOkHD0BWifLJppSkTt+Mf426JFk70MEsG9I6cGuCU4ZOfGOS0OiKVXy9yHXIElAKS0thcFggNFoxLJly/Djjz8CAEwmE8xmM2JiYsR1PTw8MGPGDJw9e1aKUjRNytETq91rdmBD3nmOoJDk2DekkbzmCiasGiz5+6QEj0B+whPw35GGrNFlyBpdxrBC/eLwvzqTJ0/Ghx9+iGPHjuHgwYMwm82IiopCTU0NzGYzACAwMNDmOYGBgeKyzjQ1NcFisdjcXJ13xFZsq/pPyd8nJXgEskaXYc+pLyV/L3JdUvQNgL1Drnkn16cvwvXpi+C/gyMq1HcOP4snNjZW/HncuHGYOnUqnnjiCXzwwQeYMmUKAECn09k8RxCEDo+1lZKSgu3btzu6VFW72LgOGfELnfJe16cvgv/0Rcg6ndvp8qLjR7HfuNYptZA2SdE3APaOcf/xW5SM9ZO1Bv8daUh8czV7BNlN8nH7IUOGYNy4cSgtLRVn5bff66muru6wd9RWcnIy6urqxFtFRYWkNVPnrHtF7W/cSyJHc0TfAFy7dySvuYIBgcoYMbL2CPYJsofkAaWpqQklJSUICgqC0WiEXq9Hfv7j72hobm5GQUEBoqKiunwNDw8P+Pj42NxIWfx3pMldAmmII/oG4Nq940DBh/jqq6/kLkPkvyONOzNkF4cHlM2bN6OgoAAmkwl///vf8bvf/Q4WiwUrV66ETqdDUlISdu7cidzcXFy6dAmrVq3C4MGDsWLFCkeXolnOmn9iLzYe6iv2DcerL9mFmKeXyF1GB9aQkj3VKHcppHAOn4NSWVmJ5cuX49atWxg+fDimTJmCoqIihIWFAQBee+01NDQ0YN26dbhz5w4mT56M48ePw9ubX81tj62TX8QhuYtox39HGrKv1GLFtya5SyGVYd+Qxiu1S/Fi1osIi18tdyk2/Hek4RqAxOxUTF3xNnsGdUonCIIgdxH2slgs8PX1RUjSpxjgIf3pc0rjHbEV50Zki9+XozRPMaS4hEdN91Hx7hLU1dWp5tCJK/YO74itWHUkDPURExUXVKyeulILAOwbLsCevsHv4lGhNd/+HxzapMxwAgDXxvohG0Y2GyIFqC/Zhf1GYIvZS+5SunTtlzONsvH4sA/7BzGgqEzZnEI0tqxQ3OGd9hhSiMhe19qcEm0NK+whrouXB1URMZwo9NBOe9fG+iF7qhHZU43YUqvcvTciUp5rY/1wbawfe4cL4wiKQnhHbMXFkfMRfiK60+VqCydW1j2iwWP9sGXPVfHx3X4NMlVE5HrUMvLamcGbxtj0jrbYR7SNAUUh3n+wBmfvPQeg8+HMs/d+bzP8qUaDN40Rf96y5yqbC5ETqHXnpq22vaMta3BhL9EmHuJRkGfcH6JsTmGHx8vmFOIZ94cyVCSdwZvGYEutF4dviSR29t7vVR1OujN40xj2Eg3jCIqTJK+5gleO/arL5dsq/onr0wOw/MYKlBltQ4ra9366Yt0rKjv1BcJNzv0yMyJX0Dp6EoprchciMWsv4cistjCg9MPRvM24mvkNNuSd7zGAFJmnIWNW13+Ew37576FRAcAo53wJoFJkzFrIkEKKV2YsB4Auf0+P5m1GxIb/0eHx3PGLsSHvvKS1dUYLh3bsNXjTGPYSDWFAsUP7BnTjry8j8HINyozluPFgIDJmuVawcCSGFFISaxhpy/rvu+zUF50+R596CCnBIzo8/tSVmk5frzOOCjOuGE6srL3Eij1FvVwmoBzN24zYuHfE//bl+c+1b0ACgLHAtbELW3+mfmFIIaXobmejq2UpXazferps73Zebr+ZgLJZv229U7UV4Y3ZvXpeW64cTqzafkbsKerlMgHFOy4NgAVFh0/26Rf29pb1KGscCNyoER8L93zY6R4T9V3GrIU4umC2eL8vYZKor47mbQYAFCWclOX9/XekIeOXn2+/+SWO/sdmRCz72SaoeEdsxUVTeafhheGkI+74qJfLBJRoWHA0bzOKEk62puv0YrueH18aDpS2DtMmr7mC6c//PxT+9WWcEZZJUK1rKzr8+I+DNawwqJAztP3dk5v/jjQUATjy5moxqIwzhuJF04v4W8hwlFWs6BBS9hy8CP8dDCftMaSok8sEFAAw7vsURQ54nVE/vo+iw2d4WMcJrH8wji6YzZBCLskaVG6fzsWLpn8iLH41rgPAadiElNbRkxRVXozNGdqOzrKXqINLBZQxBeWwXo9wex9fI3nNFYQ9eBXlDCdOVXT4pN2jXkRacn36IvFsP+v9tiGFoyc9a7vDY8WwolwuFVCA1pDSH5EnByP6vzyLlGAHFUS91tcJzkRadX36Iny3JwJlv8vn6Ikd2h9GPv3Vf0fK+2NlrIg6wyvJ2im+NBxZwotyl+GSig6fRPKaK0hec0XuUkijbmalyV2C3X4zqYQTY/uh6PBJuLfE42jeZvYXhXG5ERRHeOp4JsoT5K7CNbm3xAMAktdk2TzOvR9yhMzGSViVlYaw+NVyl9Jr16cvap2TQv3SGlRafz6ax1EVJeAISh9Ew4LQimlyl+HS3FvibW7W00OJ+iuzcRJuZqWpcjSFHKPtqArJhyMoffTU8UwMiStHXIMnr4WiAEWHTyLZLQsHCj7sdHl9yS4nV0Rqltk4CQCwKisNzz//fK+fVxLA01i1pOjwSRxdMBtLklv/VLKPOBcDSh9FwwLkAefhg8q4RgYVBXBvicfGqPhOl01J4WnKZL/MxknIzKvpecVfXJ0xBN8NvQeAYUUrig6fxMZffv5DI6+l4kw8xNNP0bBgWV4Nzh+rR3JltdzlUBeKDp/sdLi2bE5hJ2sT9c2YgnIsy6vBsrwaLL9Rg4iacvFG6pcxayHKjOXwjtgqdykugSMoDhINC84fA0dTFKztcG19yS6UzSnEyQGfoswYyr0icrj2lzRoO7oCcIRFrTJmLcRGLORoihMwoDiQ9bDPZ6PL0CjsRbzuMwYVhbEO105JmY0bL72McuEMMkL4hWIkvfaBJSeOIUXNePl86fEQjwTiS8ORcPxPWFOQJ3cp1IWiwydtvkfJOnRL5CzBeS08G1DlrH2jbE4hDxdLgCMoEpobMhyNFdM4kqIS3CMiZ4qGBTj+JxTiB1yLWYXykDNyl0R9kDFrofhzmZH9w5E4giIh60hKWeNAuUuhXsqYtVC8kmSZ5wqZqyFXEA0LEo7/CXPTf5C7FOqntiMq1H8cQXGCCz+fRrLnsxxFUYlXjv0Kr8wpRNGvjmFL5n28+rt8h7xu7q+SsCHvvENei7QnGhYUpnM0Re2sIypbaq/i1d/lI/xEtMwVqRcDihPEl4bjM/wN14MXyV0K9ULbIdvBm4AMjHHI695+MwEwrnXIa5E2WQ/7rPJU1+X2qaPBm8YgA2Nag4p+EcIbs+UuSXUYUJwkvjQcWcht/Yp0IqJuvNQ4G44ZtyO5tQaVEmzZ0xpUpJT7/HWnjNJaD39LHboYUJyIIYWIesN6uCc/4Qm5SyEHsQYVKd3MSkWZ5z4AtmFlX9zEPgWXtvPwrGGkbE4hMsJat6MsK8JmmaMxoDhZfGk4CkvZeFzRUuPr2OQ5X7zvrL0dUqdoWHD1Rg0OjQqQuxRSibD41chA66HBiBoTyjxXoCTHgLq4DLsn/Y8zhiIjqjWIJFdWw1wwA4heh4ywRHGdjPhfAld6sWM2oB0GFBncHP09AAYUV9MaSh/vQc1Nvy42DUeFlfZNiCFI3W4V1AIMKNQHJQGhKIkvAX75erKSePtGbza2+TkleMTjMNKJMs8VkoyiMKDIgId6CLANLMtvVAN9HIYFWodw/9e1OeIej1VyZRVS4yYCAIOKCnEUhdQgI75EklEUXgdFJvGl4bj9JmfpU6tDowIQe6ca++Imirfe2Bc3Ed4RW1Eb8BAbo451WJ4SPAK1AQ+xtqHKrtcl5bhVUMsvIiWXxBEUGe03rkXim6uxe80OXiOFftlLfggAuJmVhn1x3Z+SXK3/ELUtT3caTNpLCR6BWjxEcmW1OKLSmbaT6rpaRs4VDQs8j1kwF/Wcu0YuhQFFZvuNa+F57Cc8OfpvPORDorD41aj9Jax0xb0l3u7XtQaVnqxtqLIJzcmV1fC0+93IkTh3jVwND/EoRNHxo3KXQCRK9Qqyue957CeZKiGr+NJwPHk6V+4yiJyGAUUh9hvXck4KKcbahiokV1aLN1IGhhRyJQwoCmINKfyDQHJL9QqC57GfxP+ScjCkkKtgQFEY65yUp67Uyl0KubDagNZ5KmsbqmSuhDrDkEKuwO6Acvr0aSxYsAAGgwE6nQ55eXk2ywVBwLZt22AwGODl5YWZM2fi8uXLNus0NTUhMTERw4YNw5AhQ/DCCy+gsrKyXxuiNSu+NTGkkNM1zhsJAOIonqNGT9g3HC++NBzb04t5aJg0y+6Acu/ePYwfPx4HDhzodPnbb7+NvXv34sCBAzh37hz0ej3mzp2L+vp6cZ2kpCTk5uYiJycHhYWFuHv3LubPn4+HD3s+u8CVrPjWhO3pxQwq5DTWQOLowzrsG9LZb1zLoEKapBMEQejzk3U65ObmIi4uDkDrXpDBYEBSUhJef/11AK17PYGBgfjjH/+IV155BXV1dRg+fDg++ugjLF26FADw888/IyQkBEeOHMG8efN6fF+LxQJfX1+EJH2KAR6D+1q+qmRPNeLaWD+5yyAXsL2XV4R81HQfFe8uQV1dHXx8fHr9+nL1DcD1egf7BjlTb3qHPX3DoXNQTCYTzGYzYmJixMc8PDwwY8YMnD17FgBw4cIFtLS02KxjMBgQGRkprtNeU1MTLBaLzc1VPXk6F/f3XJW7DNKwtxKexlsJTzvt/aTqGwB7Bw8Vk5o5NKCYzWYAQGBgoM3jgYGB4jKz2Qx3d3cMHTq0y3XaS0lJga+vr3gLCQlxZNmqs9uvAdvTi/GHU1/IXQpRv0nVNwD2DgAoPtr1/x8iR3L0jo0kZ/HodDqb+4IgdHisve7WSU5ORl1dnXirqKhwWK1q0VmTCTeFYnt6MbanF2PKgtkyVEXkOI7uGwB7B/B4h2Z7ejFHX0lVHBpQ9Ho9AHTYo6murhb3jvR6PZqbm3Hnzp0u12nPw8MDPj4+NjdXY20y8aXhcpdCLqC381AcQaq+AbB3tLfbr4Ejr6QaDg0oRqMRer0e+fn54mPNzc0oKChAVFQUAGDChAlwc3OzWaeqqgqXLl0S16GOsqcakTW6TO4ySEYRNeXYe7Z3k0H7y5lzUNg3nCvcFMqQoiFz03/Q7Ci63V8WePfuXdy4cUO8bzKZ8N1338Hf3x+hoaFISkrCzp07MXr0aIwePRo7d+7E4MGDsWLFCgCAr68vEhISsGnTJgQEBMDf3x+bN2/GuHHjMGfOHMdtGZEG1ZfswvaSYnhHbO3VtxgrBfuGsoSbQlF26gtkzFoodylkpydP59qMpG//5b+xce/g6ILZKDp8Upa6pGB3QDl//jyee+458f7GjRsBACtXrkRmZiZee+01NDQ0YN26dbhz5w4mT56M48ePw9vbW3zOvn37MGjQICxZsgQNDQ2YPXs2MjMzMXDgQAdsEpH21Zfswh9G7kdGWKLcpfQK+4byMKRoT2zcO0AXh2fVtlMD9PM6KHJxtWsZAMCWWi/8ZlJJt3NQjuZt1lR6JlsRNeVYllfT4fGyOYXICEtEaMU0zD59CxnxJQ55v57mofT1OihycsXe0RvsHerQfvSkr6w9w9EcfR0Uu0dQSB67/RqAHn4xu0rPyWuuwL0lXqLKSIucOUmW5KfFwwPUtfAT0Sibo/wRWAYUIhW4mZWG7Y2TOl0WfiIaQDGAP7U+8Eu4KPNc0afRFIYT12TdweEOjWt43Dc6srd3SNUz+G3GLiDl/bFOO/uDlCO8MRt/yIro9frWa2WQa0t5fyya3bLkLoNkFN6Yje3pxfCr6Xl+l5Q9gwGFSMOsjcbabJIrq23+a8VgQkTtbcg7bxNUkiurbe5L3Tc4SdaFqHEWtytZfqMGYwrK5S6j1zhJ1jWwbyiDoybIyk22LwskZasv2SUe6gmtmIY/3Nwvc0VEpHRt+waRM3GSrIuxXuhLnFCJYslOOSMibagv2YW9mMeRFHIqBhRSzSlnRCSfxzs3rbhjQ1JjQCEAXZ9y1tdTValnc9N/QDQs4v3t3axLpDTcsSGpcQ4KdSu8MbtXp5oRkesJPxGN7enFnM8mMa1MkLUXAwr1aEPeeYYUIupS+Ilou665Q/YpOn5U7hJkwYBC5GRPns7F/T1X5S6DyKHsvTAgUU84B4XISR4P04YDfg3YjQaZKyJyrPDGbCC9mHPXyCEYUKhXNuSdF3/eFzcRtQEPZaxGnYqOHwWMa+Uug0hy4Y3ZKMuKYEihfmFAIbu1DSsAAwsRdWQdTbFinyB7MaBQv23IO8/mQ0Tdar9jAzC0UPcYUMghOms+bb2V8LSTKiEiteDOTc9uv7ka+1300DADCjnF9vRihhQi6qCznRv2CgJ4mjERESnM9vSOV7Um18MRFJKdq4yu+O9IQ6ILD9cS2aO7kOIK/cLKlfuGThAEQe4i7GWxWODr64uQpE8xwGOw3OWQg7yV8DT8agb2OJ+lrcZ5I5ESPELCqhxPK8eUHzXdR8W7S1BXVwcfHx+5y+kV9g5tcoXA4op9g4d4SDHsDScAkOoVBL+agUiurJaoKsfz35GGrNFlcpdBpBl+NQPFm1ZNiYmVuwSnY0AhxbA3nFifsyHvPFK9glQVUq5PX8SQQuQg1j5g/d6w9jctuD59ERJNqXKX4VScg0KasCHvPDBvpNxl2CWs9L8CsMhdBpGmdHW9la6sbahSzWFiV5uPwoBCmpHqFQTUPL6vpsZDRNLpbnQ2NW4i1laqp1dMiYnF/lK5q3AOBhTSjPZNqPClazgjLJOpGiJSgw1555EaNxGoUcdOzfXpi5AF6xePahsDChERuTRx50Ylh4ld5fAwJ8kSySQ/4QlOlCVSELVNttc6BhTSrIu34jFNlyN3Gd1q3RMiIiVQyxmB+QlPuMQZPQwopFkb8s5j1MfbFR9SiEg5NuSdR2HIKbnL6NFS4+tylyA5BhTStKLnj3GiLBHZZdTH2xU/iuIKGFBI06Z8NY8jKERkl/DGbFWMomgdAwppWnhjNkZ9vB1/yIpgUCGiXuPhYfkxoJDmhTdmI7wxW+4yiEhF2DPkx4BCREREisOAQkRE1IlRZf/kYR4ZMaAQyWjiPG+XuJ4BkRqFn4iWuwSXxoBCJKOU4BEucT0DIrXiKIp8GFCIiIi6wFEU+dgdUE6fPo0FCxbAYDBAp9MhLy/PZvmqVaug0+lsblOmTLFZp6mpCYmJiRg2bBiGDBmCF154AZWVlf3aECJSLvYNIrKX3QHl3r17GD9+PA4cONDlOs8//zyqqqrE25EjR2yWJyUlITc3Fzk5OSgsLMTdu3cxf/58PHz40P4tIFKxm1lpCPacL3cZkmPfIDXjYR55DLL3CbGxsYiNje12HQ8PD+j1+k6X1dXVIT09HR999BHmzJkDAPj4448REhKCEydOYN68efaWRKRaLzXORjgmyV2G5Ng3SM3CT0TDO2Ircp8FvzrDiSSZg/LNN99gxIgRePLJJ/Hyyy+juvrxdxpcuHABLS0tiImJER8zGAyIjIzE2bNnO329pqYmWCwWmxsRaYuj+wbA3kGOU1+yCzfL/yx3GS7F4QElNjYWWVlZOHXqFPbs2YNz585h1qxZaGpqAgCYzWa4u7tj6NChNs8LDAyE2Wzu9DVTUlLg6+sr3kJCQhxdNhHJSIq+AbB3EKmZ3Yd4erJ06VLx58jISEycOBFhYWH48ssvsXjx4i6fJwgCdDpdp8uSk5OxceNG8b7FYmGjIdIQKfoGwN5BpGaSn2YcFBSEsLAwlJaWAgD0ej2am5tx584dm/Wqq6sRGBjY6Wt4eHjAx8fH5kZE2uWIvgGwd5B2Nb50DltqveQuQ1KSB5SamhpUVFQgKCgIADBhwgS4ubkhPz9fXKeqqgqXLl1CVFSU1OUQkQqwbxB174ywDAv93OQuQ1J2H+K5e/cubty4Id43mUz47rvv4O/vD39/f2zbtg0vvvgigoKCUFZWhn/913/FsGHDsGjRIgCAr68vEhISsGnTJgQEBMDf3x+bN2/GuHHjxNn5RKQt7BtEZC+7A8r58+fx3HPPifetx3dXrlyJ1NRUXLx4ER9++CFqa2sRFBSE5557Dp988gm8vb3F5+zbtw+DBg3CkiVL0NDQgNmzZyMzMxMDBw50wCYRqcPNrDTERnwGlOySuxTJsW8Qkb3sDigzZ86EIAhdLj927FiPr+Hp6Yn9+/dj//799r49kWa81DgbmSXavwYKwL5BRPbjd/EQERGR4jCgEBERkeIwoBAREfUg0ZSKeN1ncpfhUhhQiIiIevCbV0qQEjxC7jJcCgMKERERKQ4DChERESkOAwoREREpDgMKERERKQ4DChERESkOAwq5jGPv/AZPXalFcmW13KUQkYpsqfXiKcYyYEAhl7HbrwHFR81yl0FEKjNv83c8xVgGDChERERd2FLrheiKWXKX0UFoxTTU562WuwxJMaCQS9nt14D/+cltHuYhol5R6ujJU8czERv3jtxlSIoBhVyONaTcfnM1br+5mmGFiEiBBsldAJEcdvs1AH5rAaA1rNx5U1y28H/PwhlhmaTv3zo86w5ofA+ISM3KjOXQV8xCSrDclbgmBhRyeW3DCgDUf3wOYfHSvqenaSNi48KlfRMi6pcbsxuQIijv8I6r4CEeIiIiUhwGFKJ2hpunY5ouR+4yiIhcGgMKUTu7/Rrw8ccWyV4/tGIann3Af3pEarP8Rk2Pk+pvZqU5ZQenMs4N3hFbJX8fObFLEjmZp2kjwk2hcpdBRHb6R/NAFIac6nadoFVDcOyd30geUkoCQnG05D1J30NuDChEnYh5egkP8xC5uP8s8LC5f22sX7dn+N3MSsMrN25jt1+DU0KK1jGgEHVixbcmSQ/zEJHyxZeG2xUyglYNQfiJaAAQQ8rc9B/Em/W1ImrKbe5T5xhQiLoQ8/QSLL9Rg+U3avrdSEIrpomvxfknRNpjHT1pa7dfA6JhEW/H3vkNlt+owX/ozyAaFow66cWQ0g1eB4WoCyu+NYk/X8VcYFTfXie0Yhpml25GuKnc+kj/iyMiRfnM+BkyT+zqdp3dfg3YXVAOFIwFAISbQlF2shyYndOni0NWxrnB+9pW1Jd0/75qxV05Igk9DicMJURaFVFTjosj5/fpueGmUIw66dWnkVqtT5RlQCGS0Hf/N4LhhEjDImrKMeXuF+Lck74IN4ViTEE5D/m0w4BC1Av/aB5o15cKhlZMw82sNGya9VsJqyIiqXn++6QulzkinLRlHU1hSGnFgELUCyu+NaGscWCv1p2my4FhnTsyGydx9IRI5aJh6TIwvHztZYeFE6twUyi+eKP7a624CgYUIgervvw8YvktxUSa0d0oihQ2zfotR1HAgELkUNN0ORj6+otyl0FEDhQNC+am/yDej6gpx82stD5PjO0JD/W04mnGRL3058xkPLvhzW7XaVl1kKMnRBoUDQsK039AZZwbpnw1D8sas5F5Qrr3CzeFIvuyERgr3XsoHQMKUS/tN67F/rya7ldiOCHSrGhYgDwAyHbK+0WZCzAg8GkMG3a2T9dJUTsGFCIiIgUKN4UCphqUGb36fDE3NeMcFCIiIgULN4WiZf7BTuek/LvnSZR5rpChKukxoBARESlcbNw7aJl/EKEV02yCSlj8ajROLpSxMukwoBAREalAbNw7SDj+pw6jKbv+/pkmR1EYUIiIiFTEOppiDSnvz4jD30J2ylyV4zGgEBERqUzbQz5b3n8T8aXhcpfkcDyLh4iISIVi494BjgMwyl2JNOwaQUlJScGkSZPg7e2NESNGIC4uDteuXbNZRxAEbNu2DQaDAV5eXpg5cyYuX75ss05TUxMSExMxbNgwDBkyBC+88AIqKyv7vzVEpEjsHURkL7sCSkFBAV599VUUFRUhPz8fDx48QExMDO7duyeu8/bbb2Pv3r04cOAAzp07B71ej7lz56K+vl5cJykpCbm5ucjJyUFhYSHu3r2L+fPn4+HDh47bMiJSDPYOIrKXThAEoa9P/uc//4kRI0agoKAA06dPhyAIMBgMSEpKwuuvvw6gdY8nMDAQf/zjH/HKK6+grq4Ow4cPx0cffYSlS5cCAH7++WeEhITgyJEjmDdvXo/va7FY4Ovri5CkTzHAY3BfyyeifnjUdB8V7y5BXV0dfHx87HoueweRa7Knb/RrkmxdXR0AwN/fHwBgMplgNpsRExMjruPh4YEZM2bg7NmzAIALFy6gpaXFZh2DwYDIyEhxHSLSNvYOIupJnyfJCoKAjRs3Ijo6GpGRkQAAs9kMAAgMDLRZNzAwEDdv3hTXcXd3x9ChQzusY31+e01NTWhqahLvWyyWvpZNRDJj7yCi3ujzCMr69evx/fff49ChQx2W6XQ6m/uCIHR4rL3u1klJSYGvr694CwkJ6WvZRCQz9g4i6o0+BZTExET85S9/wddff43g4GDxcb1eDwAd9maqq6vFPSO9Xo/m5mbcuXOny3XaS05ORl1dnXirqKjoS9lEJDP2DiLqLbsCiiAIWL9+PT7//HOcOnUKRqPtyddGoxF6vR75+fniY83NzSgoKEBUVBQAYMKECXBzc7NZp6qqCpcuXRLXac/DwwM+Pj42NyJSD/YOIrKXXXNQXn31VWRnZ+OLL76At7e3uLfj6+sLLy8v6HQ6JCUlYefOnRg9ejRGjx6NnTt3YvDgwVixYoW4bkJCAjZt2oSAgAD4+/tj8+bNGDduHObMmeP4LSQi2bF3EJG97AooqampAICZM2faPJ6RkYFVq1YBAF577TU0NDRg3bp1uHPnDiZPnozjx4/D29tbXH/fvn0YNGgQlixZgoaGBsyePRuZmZkYOHBg/7aGiBSJvYOI7NWv66DIhdcyIJJff66DIhf2DiJ5Oe06KERERERSYEAhIiIixWFAISIiIsVhQCEiIiLFYUAhIiIixWFAISIiIsVhQCEiIiLFYUAhIiIixWFAISIiIsVhQCEiIiLFYUAhIiIixWFAISIiIsVhQCEiIiLFYUAhIiIixWFAISIiIsVhQCEiIiLFGSR3AX0hCAIA4FHTfZkrIXJd1n9/1n+PasDeQSQve/qGKgNKfX09AOCn1FXyFkJEqK+vh6+vr9xl9EpNTQ0A9g4iufWmb+gENe3+/OLRo0e4du0axo4di4qKCvj4+MhdUp9ZLBaEhIRwOxREK9si9XYIgoD6+noYDAYMGKCOo8W1tbUYOnQoysvLVROqusLfU2XRynYA0m6LPX1DlSMoAwYMwMiRIwEAPj4+qv9lALgdSqSVbZFyO9T2R97aEH19fTXx2QL8PVUarWwHIN229LZvqGO3h4iIiFwKAwoREREpjmoDioeHB9566y14eHjIXUq/cDuURyvbopXtcCQt/T/RyrZwO5RHKduiykmyREREpG2qHUEhIiIi7WJAISIiIsVhQCEiIiLFYUAhIiIixVFlQHnvvfdgNBrh6emJCRMm4MyZM3KX1K1t27ZBp9PZ3PR6vbhcEARs27YNBoMBXl5emDlzJi5fvixjxY+dPn0aCxYsgMFggE6nQ15ens3y3tTe1NSExMREDBs2DEOGDMELL7yAyspKJ25Fz9uxatWqDp/RlClTbNZRwnakpKRg0qRJ8Pb2xogRIxAXF4dr167ZrKOWz0QO7B3OoZW+AWijd6i1b6guoHzyySdISkrCG2+8geLiYkybNg2xsbEoLy+Xu7Ru/frXv0ZVVZV4u3jxorjs7bffxt69e3HgwAGcO3cOer0ec+fOFb9zSE737t3D+PHjceDAgU6X96b2pKQk5ObmIicnB4WFhbh79y7mz5+Phw8fOmszetwOAHj++edtPqMjR47YLFfCdhQUFODVV19FUVER8vPz8eDBA8TExODevXviOmr5TJyNvcN5tNI3AG30DtX2DUFl/uVf/kVYs2aNzWNjxowRtm7dKlNFPXvrrbeE8ePHd7rs0aNHgl6vF3bt2iU+1tjYKPj6+grvv/++kyrsHQBCbm6ueL83tdfW1gpubm5CTk6OuM5PP/0kDBgwQPjqq6+cVntb7bdDEARh5cqVwsKFC7t8jhK3QxAEobq6WgAgFBQUCIKg3s/EGdg75KGVviEI2ukdaukbqhpBaW5uxoULFxATE2PzeExMDM6ePStTVb1TWloKg8EAo9GIZcuW4ccffwQAmEwmmM1mm23y8PDAjBkzFL9Nvan9woULaGlpsVnHYDAgMjJScdv3zTffYMSIEXjyySfx8ssvo7q6Wlym1O2oq6sDAPj7+wPQ3mfiKOwdyqHF31G19Q619A1VBZRbt27h4cOHCAwMtHk8MDAQZrNZpqp6NnnyZHz44Yc4duwYDh48CLPZjKioKNTU1Ih1q22bAPSqdrPZDHd3dwwdOrTLdZQgNjYWWVlZOHXqFPbs2YNz585h1qxZaGpqAqDM7RAEARs3bkR0dDQiIyMBaOszcST2DuXQ2u+o2nqHmvqGKr/NWKfT2dwXBKHDY0oSGxsr/jxu3DhMnToVTzzxBD744ANxMpXatqmtvtSutO1bunSp+HNkZCQmTpyIsLAwfPnll1i8eHGXz5NzO9avX4/vv/8ehYWFHZZp4TORgtr+nWm5d2jld1RtvUNNfUNVIyjDhg3DwIEDO6S16urqDslPyYYMGYJx48ahtLRUnJGvxm3qTe16vR7Nzc24c+dOl+soUVBQEMLCwlBaWgpAeduRmJiIv/zlL/j6668RHBwsPq7lz6Q/2DuUQ+u/o0ruHWrrG6oKKO7u7pgwYQLy8/NtHs/Pz0dUVJRMVdmvqakJJSUlCAoKgtFohF6vt9mm5uZmFBQUKH6belP7hAkT4ObmZrNOVVUVLl26pOjtq6mpQUVFBYKCggAoZzsEQcD69evx+eef49SpUzAajTbLtfyZ9Ad7h3Jo/XdUib1DtX1Dkqm3EsrJyRHc3NyE9PR04cqVK0JSUpIwZMgQoaysTO7SurRp0ybhm2++EX788UehqKhImD9/vuDt7S3WvGvXLsHX11f4/PPPhYsXLwrLly8XgoKCBIvFInPlglBfXy8UFxcLxcXFAgBh7969QnFxsXDz5k1BEHpX+5o1a4Tg4GDhxIkTwj/+8Q9h1qxZwvjx44UHDx4oYjvq6+uFTZs2CWfPnhVMJpPw9ddfC1OnThVGjhypuO1Yu3at4OvrK3zzzTdCVVWVeLt//764jlo+E2dj73AerfSNnrZFLb1DrX1DdQFFEAThz3/+sxAWFia4u7sLzzzzjHiqlFItXbpUCAoKEtzc3ASDwSAsXrxYuHz5srj80aNHwltvvSXo9XrBw8NDmD59unDx4kUZK37s66+/FgB0uK1cuVIQhN7V3tDQIKxfv17w9/cXvLy8hPnz5wvl5eWK2Y779+8LMTExwvDhwwU3NzchNDRUWLlyZYcalbAdnW0DACEjI0NcRy2fiRzYO5xDK32jp21RS+9Qa9/Q/VI8ERERkWKoag4KERERuQYGFCIiIlIcBhQiIiJSHAYUIiIiUhwGFCIiIlIcBhQiIiJSHAYUIiIiUhwGFCIiIlIcBhQiIiJSHAYUIiIiUhwGFCIiIlIcBhQiIiJSnP8PUA3DDAamIdEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "save_path = 'fcn/final21_lr-1234.pt'\n",
    "model = FCN().to(device)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "with torch.no_grad():\n",
    "    for index, (input, output) in enumerate(trainloader):\n",
    "        input = input.to('cuda')\n",
    "        output = output.to('cuda')\n",
    "        prediction = model(input)\n",
    "        prediction = prediction.cpu()\n",
    "        prediction = prediction[0]\n",
    "        prediction = torch.argmax(prediction, dim=0)\n",
    "        '''\n",
    "        for y in prediction:\n",
    "            for x in y:\n",
    "                print(x.item(), end='')\n",
    "            print()\n",
    "        print()\n",
    "        for y in output[0][0]:\n",
    "            for x in y:\n",
    "                print(x.item(), end='')\n",
    "            print()\n",
    "        break\n",
    "        '''\n",
    "        cmap = plt.cm.get_cmap('tab20', 20)\n",
    "        plt.subplot(121)\n",
    "        for i in range(20):\n",
    "            plt.imshow(prediction, cmap=cmap, alpha=0.5)\n",
    "        plt.subplot(122)\n",
    "        for i in range(20):\n",
    "            plt.imshow(output.cpu()[0].squeeze(), cmap=cmap, alpha=0.5)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
