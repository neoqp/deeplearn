{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW 모델 직접 구현 (TF-IDF) (wikipiedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apples', 'are', 'best', 'rule', 'the', 'they', 'truly']\n",
      "[2. 2. 1. 1. 1. 1. 3.]\n"
     ]
    }
   ],
   "source": [
    "# 구두점 제거 -> 소문자화 -> 토큰화 -> 개수\n",
    "\n",
    "import re, string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "punc_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "\n",
    "def strip_punc(corpus):\n",
    "    return punc_regex.sub('', corpus)\n",
    "\n",
    "doc = \"Apples rule. Apples are the best. Truly, they are. Truly... Truly\"\n",
    "\n",
    "doc = strip_punc(doc)\n",
    "counter = Counter(doc.lower().split())\n",
    "descriptor = np.array([counter[word] for word in sorted(counter)], dtype=float)\n",
    "print(sorted(counter))\n",
    "print(descriptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_counter(doc):\n",
    "    return Counter(strip_punc(doc).lower().split())\n",
    "\n",
    "def to_vocab(counters):\n",
    "    vocab = set()\n",
    "    for counter in counters:\n",
    "        vocab.update(counter)\n",
    "    return sorted(vocab)\n",
    "\n",
    "def to_tf(counter, vocab):\n",
    "    return np.array([counter[word] for word in vocab], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'am', 'cat', 'dog', 'i', 'not']\n",
      "[[1. 1. 0. 1. 1. 0.]\n",
      " [1. 1. 1. 0. 1. 0.]\n",
      " [1. 1. 0. 1. 1. 1.]\n",
      " [1. 2. 1. 0. 2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "doc_1 = \"I am a dog.\"\n",
    "doc_2 = \"I am a cat!\"\n",
    "doc_3 = \"I am not a dog\"\n",
    "doc_4 = \"I am not a cat, am I!?!\"\n",
    "\n",
    "word_counts = [to_counter(doc) for doc in [doc_1, doc_2, doc_3, doc_4]]\n",
    "bag = to_vocab(word_counts)\n",
    "tfs = np.vstack([to_tf(counter, bag) for counter in word_counts])\n",
    "print(bag)\n",
    "print(tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 k개 추출\n",
    "# 불용어 처리\n",
    "\n",
    "def to_vocab(counters, k=None, stop_words=tuple()):\n",
    "    vocab = Counter()\n",
    "    for counter in counters:\n",
    "        vocab.update(counter)\n",
    "    \n",
    "    for word in set(stop_words):\n",
    "        if(word in counter.keys()):\n",
    "            vocab.pop(word)\n",
    "        \n",
    "    return sorted([word for (word, cnt) in vocab.most_common(k)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'after', 'all', 'also', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'first', 'for', 'from', 'had', 'has', 'have', 'he', 'his', 'in', 'into', 'is', 'it', 'its', 'many', 'more', 'most', 'new', 'not', 'of', 'on', 'one', 'or', 'other', 'some', 'such', 'that', 'the', 'their', 'there', 'they', 'this', 'to', 'was', 'were', 'which', 'who', 'with']\n"
     ]
    }
   ],
   "source": [
    "# wiki에서 상위 50개 빈출단어 출력\n",
    "\n",
    "path = \"../../dataset/wikipedia2text-extracted.txt\"\n",
    "with open(path, \"rb\") as f:\n",
    "    wiki = f.read().decode()\n",
    "\n",
    "wiki_count = to_counter(wiki)\n",
    "wik = to_vocab([wiki_count], k=50)\n",
    "print(wik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'able', 'about', 'above', 'according'] ['your', 'yours', 'yourself', 'yourselves', 'zero']\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../dataset/stopwords.txt\", 'r') as r:\n",
    "    stops = []\n",
    "    for line in r:\n",
    "        stops += [i.strip() for i in line.split('\\t')]\n",
    "        \n",
    "print(stops[:5], stops[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american', 'area', 'began', 'british', 'called', 'century', 'city', 'country', 'due', 'early', 'form', 'found', 'french', 'government', 'great', 'high', 'i', 'important', 'include', 'including', 'international', 'large', 'largest', 'life', 'made', 'major', 'million', 'modern', 'music', 'national', 'north', 'number', 'part', 'people', 'political', 'population', 'power', 'public', 'river', 'south', 'state', 'states', 'system', 'time', 'united', 'war', 'work', 'world', 'year', 'years']\n"
     ]
    }
   ],
   "source": [
    "wiki_count = to_counter(wiki)\n",
    "wik2 = to_vocab([wiki_count], k=50, stop_words=stops)\n",
    "print(wik2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 0., 2.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_1 = \"I am a dog\"\n",
    "doc_2 = \"I am a cat!\"\n",
    "doc_3 = \"I am not a dog?\"\n",
    "doc_4 = \"I am not a cat, am I!?!\"\n",
    "\n",
    "word_counts = [to_counter(doc) for doc in [doc_1, doc_2, doc_3, doc_4]]\n",
    "vocab = to_vocab(word_counts, stop_words=stops)\n",
    "tfs = np.vstack([to_tf(counter, vocab) for counter in word_counts])\n",
    "tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tf(counter, vocab):\n",
    "    x = np.array([counter[word] for word in vocab], dtype=float)\n",
    "    return x / x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.5       , 0.5       ],\n",
       "       [0.5       , 0.        , 0.5       ],\n",
       "       [0.        , 0.5       , 0.5       ],\n",
       "       [0.33333333, 0.        , 0.66666667]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_1 = \"I am a dog\"\n",
    "doc_2 = \"I am a cat!\"\n",
    "doc_3 = \"I am not a dog?\"\n",
    "doc_4 = \"I am not a cat, am I!?!\"\n",
    "\n",
    "word_counts = [to_counter(doc) for doc in [doc_1, doc_2, doc_3, doc_4]]\n",
    "vocab = to_vocab(word_counts, stop_words=stops)\n",
    "tfs = np.vstack([to_tf(counter, vocab) for counter in word_counts])\n",
    "tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_idf(vocab, counters):\n",
    "    N = len(counters)\n",
    "    nt = [sum(1 if t in counter else 0 for counter in counters) for t in vocab]\n",
    "    nt = np.array(nt, dtype=float)\n",
    "    return np.log10(N / nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.25       0.         0.25       0.25       0.\n",
      "  0.         0.25       0.         0.        ]\n",
      " [0.14285714 0.28571429 0.         0.14285714 0.         0.\n",
      "  0.14285714 0.         0.14285714 0.14285714]\n",
      " [0.         0.         0.33333333 0.         0.33333333 0.33333333\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.25       0.         0.25       0.         0.5\n",
      "  0.         0.         0.         0.        ]]\n",
      "[0.60205999 0.12493874 0.60205999 0.12493874 0.30103    0.30103\n",
      " 0.60205999 0.60205999 0.60205999 0.60205999]\n",
      "[[0.         0.03123468 0.         0.03123468 0.0752575  0.\n",
      "  0.         0.150515   0.         0.        ]\n",
      " [0.08600857 0.03569678 0.         0.01784839 0.         0.\n",
      "  0.08600857 0.         0.08600857 0.08600857]\n",
      " [0.         0.         0.20068666 0.         0.10034333 0.10034333\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.03123468 0.         0.03123468 0.         0.150515\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "doc_1 = \"Apple cider is delicious.\"\n",
    "doc_2 = \"A recipe for apple cider, using apple.\"\n",
    "doc_3 = \"Donuts are delicious\"\n",
    "doc_4 = \"Apple cider donuts, anyone? Donuts?\"\n",
    "\n",
    "word_counts = [to_counter(doc) for doc in [doc_1, doc_2, doc_3, doc_4]]\n",
    "vocab = to_vocab(word_counts, stop_words=stops)\n",
    "tfs = np.vstack([to_tf(counter, vocab) for counter in word_counts])\n",
    "idf = to_idf(vocab, word_counts)\n",
    "tf_idfs = tfs * idf\n",
    "\n",
    "print(tfs)\n",
    "print(idf)\n",
    "print(tf_idfs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
